{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce8ed7c",
   "metadata": {},
   "source": [
    "# Project Phase 2\n",
    "## Mohammad Amin Rami 98101588\n",
    "## Milad Heidari 98101469\n",
    "## Mohammad Reza Safavi 98106701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce99141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103e66a",
   "metadata": {},
   "source": [
    "### A custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e11ab463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCTD(Dataset):\n",
    "    def __init__(self, root='data', mode='train', transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_dir = os.path.join(self.root, 'MSCTD', self.mode, 'texts')\n",
    "        self.texts = []\n",
    "        self.targets = []\n",
    "        self.read_data()\n",
    "\n",
    "    \n",
    "    def read_data(self):\n",
    "        with open(os.path.join(self.data_dir, f'english_{self.mode}.txt')) as file:\n",
    "            for sentence in file:\n",
    "                sentence = MSCTD.pre_processing(sentence)\n",
    "                self.texts.append(sentence)\n",
    "                \n",
    "        with open(os.path.join(self.data_dir, f'sentiment_{self.mode}.txt')) as file:\n",
    "            for sentiment in file:\n",
    "                self.targets.append(int(sentiment.strip()))   \n",
    "    \n",
    "    @staticmethod\n",
    "    def pre_processing(sentence: str):\n",
    "        #punc_tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        sentence = sentence.strip().lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        #sentence = punc_tokenizer.tokenize(sentence)\n",
    "        sentence = sentence.split(' ')\n",
    "        sentence = [word for word in sentence if word not in stopwords.words('english')]\n",
    "        sentence = \" \".join(sentence)\n",
    "        return sentence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        target = self.targets[index]\n",
    "        if self.transform is not None:\n",
    "            text = self.transform(text)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return (text, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4539720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MSCTD()\n",
    "test_data = MSCTD(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a7ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_data.texts)\n",
    "train_data.transform = lambda x: torch.Tensor(vectorizer.transform([x]).toarray()).to(device).reshape((-1, ))\n",
    "test_data.transform = lambda x: torch.Tensor(vectorizer.transform([x]).toarray()).to(device).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05cb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9f74c",
   "metadata": {},
   "source": [
    "### TF-IDf based classification of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32824114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82f38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vectorizer.get_feature_names_out())\n",
    "model = MLP(input_dim=input_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51e123e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (stack): Sequential(\n",
       "    (0): Linear(in_features=9416, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8879e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fe5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    k = int(size/dataloader.batch_size/5)\n",
    "\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % k == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90517d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.091187  [    0/20240]\n",
      "loss: 1.102074  [ 4032/20240]\n",
      "loss: 1.069897  [ 8064/20240]\n",
      "loss: 1.082272  [12096/20240]\n",
      "loss: 1.125325  [16128/20240]\n",
      "loss: 1.066297  [20160/20240]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 1.077940 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.077286  [    0/20240]\n",
      "loss: 1.052224  [ 4032/20240]\n",
      "loss: 1.021806  [ 8064/20240]\n",
      "loss: 0.996611  [12096/20240]\n",
      "loss: 1.026363  [16128/20240]\n",
      "loss: 1.021116  [20160/20240]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.007037 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    try:\n",
    "        train_loop(train_loader, model, loss_fn, optimizer)\n",
    "        test_loop(test_loader, model, loss_fn)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Training interrupted')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bd021",
   "metadata": {},
   "source": [
    "### Classification using GloVe vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a7b834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        output, hidden = self.lstm(x, h)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "679fe763",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "with open('GloVe/glove.6B.50d.txt') as file:\n",
    "    for line in file:\n",
    "        data = line.split(' ')\n",
    "        word = data[0]\n",
    "        tensor = torch.Tensor([float(num) for num in data[1:]]).to(device)\n",
    "        tensor = tensor.reshape((1, -1))\n",
    "        glove_dict[word] = tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "14524f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word in glove_dict]\n",
    "    if len(words) == 0:\n",
    "        return torch.zeros_like(glove_dict['the'])\n",
    "    return torch.cat([glove_dict[word] for word in words], dim=0)\n",
    "\n",
    "train_data.transform = sentence2tensor\n",
    "test_data.transform = sentence2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9b375984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    k = int(size/dataloader.batch_size/5)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        hidden_state = (torch.zeros((1, model.hidden_size)).to(device), torch.zeros((1, model.hidden_size)).to(device))\n",
    "        seq_len = X.shape[1]\n",
    "        y = y.to(device)\n",
    "        X = X.reshape((seq_len, model.input_size))\n",
    "        for i in range(seq_len):\n",
    "            x = torch.reshape(X[i, :], (1, -1)).to(device)\n",
    "            pred, hidden_state = model(x, hidden_state)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % k == 0:\n",
    "            loss, current = loss.item(), batch \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            hidden_state = (torch.zeros((1, model.hidden_size)).to(device), torch.zeros((1, model.hidden_size)).to(device))\n",
    "            y = y.to(device)\n",
    "            seq_len = X.shape[1]\n",
    "            X = X.reshape((seq_len, model.input_size))\n",
    "            for i in range(seq_len):\n",
    "                x = torch.reshape(X[i, :], (1, -1)).to(device)\n",
    "                pred, hidden_state = model(x, hidden_state)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "02245ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f01b9300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(50, 64)\n",
       "  (linear): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = LSTMClassifier(50, 64).to(device)\n",
    "rnn_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65ebb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "55a2a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.129162  [    0/20240]\n",
      "loss: 0.999080  [ 4048/20240]\n",
      "loss: 1.110698  [ 8096/20240]\n",
      "loss: 0.761817  [12144/20240]\n",
      "loss: 0.969027  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.022370 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.014664  [    0/20240]\n",
      "loss: 1.017623  [ 4048/20240]\n",
      "loss: 1.607960  [ 8096/20240]\n",
      "loss: 1.056317  [12144/20240]\n",
      "loss: 1.308104  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 1.014362 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.167966  [    0/20240]\n",
      "loss: 1.574766  [ 4048/20240]\n",
      "loss: 0.604439  [ 8096/20240]\n",
      "loss: 1.181397  [12144/20240]\n",
      "loss: 0.954669  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 1.001538 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.524819  [    0/20240]\n",
      "loss: 0.403674  [ 4048/20240]\n",
      "loss: 1.340206  [ 8096/20240]\n",
      "loss: 0.786581  [12144/20240]\n",
      "loss: 1.451468  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 0.990702 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.463823  [    0/20240]\n",
      "loss: 0.426727  [ 4048/20240]\n",
      "loss: 0.962935  [ 8096/20240]\n",
      "loss: 0.361514  [12144/20240]\n",
      "loss: 1.674504  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 0.997487 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.055229  [    0/20240]\n",
      "loss: 1.499079  [ 4048/20240]\n",
      "loss: 0.255452  [ 8096/20240]\n",
      "loss: 0.500079  [12144/20240]\n",
      "loss: 0.495576  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.992526 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.926591  [    0/20240]\n",
      "loss: 0.872225  [ 4048/20240]\n",
      "loss: 1.307519  [ 8096/20240]\n",
      "loss: 1.899367  [12144/20240]\n",
      "loss: 0.924570  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 0.991252 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.374306  [    0/20240]\n",
      "loss: 1.038846  [ 4048/20240]\n",
      "loss: 1.804772  [ 8096/20240]\n",
      "loss: 0.927576  [12144/20240]\n",
      "loss: 0.623930  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.989655 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.186422  [    0/20240]\n",
      "loss: 3.403180  [ 4048/20240]\n",
      "loss: 0.915240  [ 8096/20240]\n",
      "loss: 1.352568  [12144/20240]\n",
      "loss: 0.067502  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.996467 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.430518  [    0/20240]\n",
      "loss: 1.762681  [ 4048/20240]\n",
      "loss: 0.867658  [ 8096/20240]\n",
      "loss: 1.298757  [12144/20240]\n",
      "loss: 1.082616  [16192/20240]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 0.990503 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    try:\n",
    "        train_loop(train_loader, rnn_model, loss_fn, rnn_optimizer)\n",
    "        test_loop(test_loader, rnn_model, loss_fn)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Training interrupted')\n",
    "        break\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ac332",
   "metadata": {},
   "source": [
    "### Classification using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "60d6c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d473db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
